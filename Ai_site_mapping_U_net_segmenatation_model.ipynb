{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyI1tzGtU5LO",
        "outputId": "3eecad49-e912-48d6-e54c-e069eeb8297e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/154.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q albumentations segmentation-models-pytorch timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "fKTktedhVWr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n"
      ],
      "metadata": {
        "id": "IJZ-H_19aGl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cde007c2"
      },
      "source": [
        "DATASET_PATH = '/content/drive/MyDrive/Dataset(main)'\n",
        "IMAGES_PATH = os.path.join(DATASET_PATH, 'images')\n",
        "MASKS_PATH = os.path.join(DATASET_PATH, 'masks')\n",
        "\n",
        "print(f\"DATASET_PATH: {DATASET_PATH}\")\n",
        "print(f\"IMAGES_PATH: {IMAGES_PATH}\")\n",
        "print(f\"MASKS_PATH: {MASKS_PATH}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "id": "yokFeEgip6Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Rotate(limit=30, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.GaussianBlur(p=0.2),\n",
        "\n",
        "    # ðŸ”¥ CRITICAL FIX\n",
        "    A.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    A.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "_Ze_XfR1k-fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArchaeologyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, files, transforms=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.files[idx]\n",
        "\n",
        "        image = cv2.imread(os.path.join(self.image_dir, img_name))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(\n",
        "            os.path.join(self.mask_dir, img_name),\n",
        "            cv2.IMREAD_GRAYSCALE\n",
        "        )\n",
        "\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms(image=image, mask=mask)\n",
        "            image = augmented[\"image\"]\n",
        "            mask = augmented[\"mask\"]\n",
        "\n",
        "        # âœ… FIX: convert dtype only\n",
        "        mask = mask.long()\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "yPDkkJ5JpuOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_exts = (\".png\", \".jpg\", \".jpeg\")\n",
        "\n",
        "all_files = sorted([\n",
        "    f for f in os.listdir(IMAGES_PATH)\n",
        "    if f.lower().endswith(valid_exts)\n",
        "])\n",
        "\n",
        "train_files, val_files = train_test_split(\n",
        "    all_files, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_ds = ArchaeologyDataset(IMAGES_PATH, MASKS_PATH, train_files, train_transform)\n",
        "val_ds   = ArchaeologyDataset(IMAGES_PATH, MASKS_PATH, val_files, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
        "\n",
        "print(\"Train:\", len(train_ds), \"Val:\", len(val_ds))"
      ],
      "metadata": {
        "id": "TVOkzH8ypw-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e574515b"
      },
      "source": [
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=3\n",
        ").to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3c29c4b"
      },
      "source": [
        "!pip install -q albumentations segmentation-models-pytorch timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dice_loss  = smp.losses.DiceLoss(mode=\"multiclass\")\n",
        "focal_loss = smp.losses.FocalLoss(mode=\"multiclass\")\n",
        "\n",
        "def combined_loss(pred, target):\n",
        "    return dice_loss(pred, target) + focal_loss(pred, target)\n"
      ],
      "metadata": {
        "id": "hoNVr8z4mStK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_iou(pred, target, num_classes=3):\n",
        "    pred = pred.argmax(dim=1)\n",
        "    ious = []\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_c = pred == cls\n",
        "        target_c = target == cls\n",
        "\n",
        "        inter = (pred_c & target_c).sum().float()\n",
        "        union = (pred_c | target_c).sum().float()\n",
        "\n",
        "        if union == 0:\n",
        "            ious.append(torch.tensor(1.0, device=device))\n",
        "        else:\n",
        "            ious.append(inter / union)\n",
        "\n",
        "    return torch.mean(torch.stack(ious))\n"
      ],
      "metadata": {
        "id": "yISUbHYMmSe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=1e-4, weight_decay=1e-5\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=0.5, patience=3\n",
        ")\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda')"
      ],
      "metadata": {
        "id": "FxO4VqKTnMYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "best_iou = 0.0\n",
        "accumulation_steps = 4  # Effective batch size = 16\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (images, masks) in enumerate(tqdm(train_loader)):\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = combined_loss(outputs, masks)\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item() * accumulation_steps\n",
        "\n",
        "    model.eval()\n",
        "    val_iou = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            val_iou += mean_iou(outputs, masks)\n",
        "\n",
        "    val_iou /= len(val_loader)\n",
        "    scheduler.step(val_iou)\n",
        "\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    print(f\"Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "    print(f\"Val IoU: {val_iou:.4f} | LR: {lr:.2e}\")\n",
        "\n",
        "    if val_iou > best_iou:\n",
        "        best_iou = val_iou\n",
        "        torch.save(\n",
        "            model.state_dict(),\n",
        "            \"/content/drive/MyDrive/best_unet_week3_fixed.pth\"\n",
        "        )\n",
        "        print(\"âœ… Model saved!\")"
      ],
      "metadata": {
        "id": "0HYz659tTmec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None,  # IMPORTANT for inference\n",
        "    in_channels=3,\n",
        "    classes=3\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(\n",
        "    torch.load(\"/content/drive/MyDrive/best_unet_week3_fixed.pth\",\n",
        "               map_location=device)\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded\")\n"
      ],
      "metadata": {
        "id": "YTaoJxc0TmLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "infer_transform = A.Compose([\n",
        "    A.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "def predict_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    aug = infer_transform(image=image)\n",
        "    img_tensor = aug[\"image\"].unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        pred_mask = torch.argmax(output, dim=1)\n",
        "\n",
        "    return image, pred_mask.squeeze().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "_xuGN0WBTl6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_prediction(idx):\n",
        "    image_tensor, gt_mask_tensor = val_ds[idx]\n",
        "    image_display = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    gt_mask_display = gt_mask_tensor.cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(image_tensor.unsqueeze(0).to(device))\n",
        "        pred_mask_display = pred.argmax(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    ax[0].imshow(image_display)\n",
        "    ax[0].set_title(\"Original Image\")\n",
        "    ax[0].axis(\"off\")\n",
        "\n",
        "    ax[1].imshow(gt_mask_display, cmap=\"tab10\")\n",
        "    ax[1].set_title(\"Ground Truth Mask\")\n",
        "    ax[1].axis(\"off\")\n",
        "\n",
        "    ax[2].imshow(pred_mask_display, cmap=\"tab10\")\n",
        "    ax[2].set_title(\"Predicted Mask\")\n",
        "    ax[2].axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nGZ2uED-rdwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overlay_mask(image, mask, alpha=0.5):\n",
        "    colors = np.array([\n",
        "        [0, 0, 0],       # background\n",
        "        [255, 0, 0],     # class 1\n",
        "        [0, 255, 0],     # class 2\n",
        "    ])\n",
        "\n",
        "    overlay = colors[mask]\n",
        "    return cv2.addWeighted(image, 1-alpha, overlay, alpha, 0)\n",
        "\n",
        "def visualize_overlay(idx):\n",
        "    image_tensor, gt_mask_tensor = val_ds[idx]\n",
        "    image_display = image_tensor.permute(1, 2, 0).cpu().numpy().astype(np.uint8)\n",
        "    gt_mask_display = gt_mask_tensor.cpu().numpy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(image_tensor.unsqueeze(0).to(device))\n",
        "        pred_mask_display = pred.argmax(dim=1).squeeze().cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    ax[0].imshow(overlay_mask(image_display, gt_mask_display))\n",
        "    ax[0].set_title(\"GT Overlay\")\n",
        "    ax[1].imshow(overlay_mask(image_display, pred_mask_display))\n",
        "    ax[1].set_title(\"Prediction Overlay\")\n",
        "    ax[0].axis(\"off\")\n",
        "    ax[1].axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "e3a2Xl1Fy1ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_iou(loader):\n",
        "    model.eval()\n",
        "    iou = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            preds = model(images)\n",
        "            iou += mean_iou(preds, masks)\n",
        "\n",
        "    return iou / len(loader)\n",
        "\n",
        "val_iou = evaluate_iou(val_loader)\n",
        "print(f\"Mean Validation IoU: {val_iou:.4f}\")\n"
      ],
      "metadata": {
        "id": "krfWMhoLy6a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dice_metric = smp.losses.DiceLoss(mode=\"multiclass\")\n",
        "\n",
        "def evaluate_dice(loader):\n",
        "    dice = 0\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            preds = model(images)\n",
        "            dice += 1 - dice_metric(preds, masks)\n",
        "    return dice / len(loader)\n",
        "\n",
        "val_dice = evaluate_dice(val_loader)\n",
        "print(f\"Mean Validation Dice: {val_dice:.4f}\")\n"
      ],
      "metadata": {
        "id": "8NBPGUL9y6P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def per_class_iou(loader, num_classes=3):\n",
        "    ious = torch.zeros(num_classes, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            preds = model(images).argmax(dim=1)\n",
        "\n",
        "            for cls in range(num_classes):\n",
        "                inter = ((preds == cls) & (masks == cls)).sum().float()\n",
        "                union = ((preds == cls) | (masks == cls)).sum().float()\n",
        "                if union > 0:\n",
        "                    ious[cls] += inter / union\n",
        "\n",
        "    return (ious / len(loader)).cpu()\n",
        "\n",
        "class_iou = per_class_iou(val_loader)\n",
        "for i, v in enumerate(class_iou):\n",
        "    print(f\"Class {i} IoU: {v:.4f}\")\n"
      ],
      "metadata": {
        "id": "XILbL-_ny--x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b633ed8"
      },
      "source": [
        "visualize_prediction(idx=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_prediction(idx=7)\n",
        "visualize_prediction(idx=11)\n",
        "visualize_prediction(idx=16)\n",
        "visualize_prediction(idx=18)\n",
        "visualize_prediction(idx=30)"
      ],
      "metadata": {
        "id": "ZVpvAj41z3ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=None,  # IMPORTANT for inference\n",
        "    in_channels=3,\n",
        "    classes=3\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(\n",
        "    torch.load(\"/content/drive/MyDrive/best_unet_week3_fixed.pth\",\n",
        "               map_location=device)\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"âœ… Model loaded\")\n"
      ],
      "metadata": {
        "id": "8BuVIok-jdkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "inference_transform = A.Compose([\n",
        "    A.Resize(256, 256),  # same as training size\n",
        "    A.Normalize(\n",
        "        mean=(0.485, 0.456, 0.406),\n",
        "        std=(0.229, 0.224, 0.225)\n",
        "    ),\n",
        "    ToTensorV2()\n",
        "])\n"
      ],
      "metadata": {
        "id": "4FJlxII9WwVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def load_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    transformed = inference_transform(image=image)\n",
        "    image_tensor = transformed[\"image\"].unsqueeze(0)  # (1, C, H, W)\n",
        "\n",
        "    return image, image_tensor.to(device)\n"
      ],
      "metadata": {
        "id": "ofz6R0nzkSVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_mask(model, image_tensor):\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        prediction = torch.argmax(output, dim=1)  # (1, H, W)\n",
        "    return prediction.squeeze(0).cpu().numpy()\n"
      ],
      "metadata": {
        "id": "xl89FTlVkaim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_prediction(original_image, predicted_mask):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.imshow(original_image)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Predicted Mask\")\n",
        "    plt.imshow(predicted_mask, cmap=\"viridis\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Overlay\")\n",
        "    plt.imshow(original_image)\n",
        "    plt.imshow(predicted_mask, alpha=0.5, cmap=\"jet\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "0isFmEUmkbGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/ruin10.png\"  # <-- upload your image here\n",
        "\n",
        "original_image, image_tensor = load_image(image_path)\n",
        "predicted_mask = predict_mask(model, image_tensor)\n",
        "\n",
        "visualize_prediction(original_image, predicted_mask)\n"
      ],
      "metadata": {
        "id": "FbCQFZkD_6EO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}